{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image prediction (cat/dog) using CNN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NEKFaNbKAaP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5r6UeaKbxjP"
   },
   "source": [
    "**Source dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IK5kYeXfKjrW",
    "outputId": "06a70213-9355-498d-8cfe-e5caf6a33698"
   },
   "outputs": [],
   "source": [
    "data_dir = tf.keras.utils.get_file(\n",
    "    'cats_dogs',\n",
    "    'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip',\n",
    "    extract=True)\n",
    "#or use: https://www.tensorflow.org/datasets/catalog/cats_vs_dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NIy16hiAMW-t",
    "outputId": "acb835b2-c1d2-4a96-f6b0-e0c700e10b0a"
   },
   "outputs": [],
   "source": [
    "data_dir  #Average image size: 403x358.  Found 2000 images in folder - /root/.keras/datasets/cats_dogs/cats_and_dogs_filtered/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z38Ewb5WUBxd"
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 128  #target img size\n",
    "BATCH_SIZE = 16   #16 imgs as 1 batch\n",
    "train_dir = data_dir + '/cats_and_dogs_filtered/train'\n",
    "validation_dir = data_dir + '/cats_and_dogs_filtered/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qirLxmsRb5p2"
   },
   "source": [
    "**Read from dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7e1ff9d4",
    "outputId": "c0cf2a55-8c8d-490f-a9ed-d1faeff879ea"
   },
   "outputs": [],
   "source": [
    "# tf.keras.utils.image_dataset_from_directory is recommanded than ImageDataGenerator\n",
    "train_ds_base = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    interpolation='nearest',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_ds_base = tf.keras.utils.image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    interpolation='nearest',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMu9vugSo2pN",
    "outputId": "7c4701b3-bf00-44e4-8e5a-f3754bc606cd"
   },
   "outputs": [],
   "source": [
    "class_names = tuple(train_ds_base.class_names)\n",
    "train_size = train_ds_base.cardinality().numpy()\n",
    "valid_size = validation_ds_base.cardinality().numpy()\n",
    "\n",
    "class_names, train_size, valid_size   #2000/16 = 125 batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaa696d6"
   },
   "source": [
    "**Apply  preprocessing (normalize) and data augmentation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70eb825d"
   },
   "outputs": [],
   "source": [
    "normalization_layer = layers.Rescaling(1. / 255)\n",
    "augment_model = tf.keras.Sequential([\n",
    "    # layers.RandomFlip(\"horizontal\"),\n",
    "    # layers.RandomRotation(0.1),\n",
    "    # layers.RandomZoom(0.2),\n",
    "    layers.RandomTranslation(0.1, 0.1),\n",
    "    normalization_layer\n",
    "])\n",
    "\n",
    "train_ds = train_ds_base.map(lambda images, labels:\n",
    "                        (augment_model(images), labels))\n",
    "validation_ds = validation_ds_base.map(lambda images, labels:\n",
    "                        (normalization_layer(images), labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnInpaLYp4J8"
   },
   "source": [
    "**CNN architecture:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "Xy0msKccp8rK",
    "outputId": "c2608612-84ce-41ef-e2e9-eb6b17ef1956"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([  # = tf.keras.models.Sequential\n",
    "    layers.InputLayer(shape=(IMAGE_SIZE, IMAGE_SIZE, 3)),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPool2D(),\n",
    "\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    #layers.SpatialDropout2D(0.1),\n",
    "\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    #layers.SpatialDropout2D(0.1)\n",
    "])\n",
    "\n",
    "#ANN\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.1))\n",
    "model.add(layers.Dense(1, activation= tf.keras.activations.sigmoid))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YbGtBKZvXJL",
    "outputId": "a8badd04-cb49-450e-a9ee-78067bea357c"
   },
   "outputs": [],
   "source": [
    "#MBGD (BATCH_SIZE) still overrides SGD here\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.025),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(train_ds, validation_data=validation_ds, epochs=30,\n",
    "                    steps_per_epoch=train_size, validation_steps=valid_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_9rkE8NbeOm"
   },
   "source": [
    "**Plot accuracy and loss:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "id": "sDB4bnTTVFYN",
    "outputId": "d564629e-4e60-4fbe-b228-281f4b313d48"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5275148"
   },
   "source": [
    "**Test:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9HKgkd4eJjW"
   },
   "outputs": [],
   "source": [
    "def predict_image(img_path):\n",
    "  img = tf.keras.utils.load_img(img_path, target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "  img_array = tf.keras.utils.img_to_array(img)\n",
    "  img_array = tf.expand_dims(img_array, 0)  # Add batch dimension\n",
    "  img_array = img_array / 255.0  # Normalize\n",
    "\n",
    "  prediction = model.predict(img_array)\n",
    "\n",
    "  predicted_class_index = (prediction > 0.5).astype(int)[0][0]\n",
    "  predicted_class_name = class_names[predicted_class_index]\n",
    "  print(f\"The predicted class is: {predicted_class_name} (score:{prediction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cb9a660",
    "outputId": "52b903f8-b7f5-4f65-b6ea-d565f46c3062"
   },
   "outputs": [],
   "source": [
    "#Test cat image\n",
    "img_path = \"/root/.keras/datasets/cats_dogs/cats_and_dogs_filtered/validation/cats/cat.2036.jpg\"\n",
    "predict_image(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7YAnJrEaj3m",
    "outputId": "2268efd4-c628-40bf-f55b-7d34036c6bbe"
   },
   "outputs": [],
   "source": [
    "#Test dog image\n",
    "img_path = \"/root/.keras/datasets/cats_dogs/cats_and_dogs_filtered/validation/dogs/dog.2024.jpg\"\n",
    "predict_image(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpwSVuvTffK3"
   },
   "source": [
    "**System** - predict new uploaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101,
     "referenced_widgets": [
      "a6f1e8f9ba1d45da87f0a9c10666bd5b",
      "95c660948d89422c95cefb98127aaa9b",
      "ce4fc228205d4807bc1b856ab49442c0"
     ]
    },
    "id": "164826fe",
    "outputId": "f45e17c8-3476-4dae-831d-80e98de03dfd"
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "upload_button = widgets.FileUpload(\n",
    "    accept='image/*',\n",
    "    multiple=False  # Allow only one file upload at a time\n",
    ")\n",
    "\n",
    "def on_upload_change(change):\n",
    "    for name, file_info in upload_button.value.items():\n",
    "        img_path = name\n",
    "        # Save the uploaded file to a temporary location\n",
    "        with open(img_path, 'wb') as f:\n",
    "            f.write(file_info['content'])\n",
    "\n",
    "        # Call the prediction function with the uploaded image path\n",
    "        predict_image(img_path)\n",
    "\n",
    "upload_button.observe(on_upload_change, names='value')\n",
    "\n",
    "print(\"Upload an image to predict:\")\n",
    "display(upload_button)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "95c660948d89422c95cefb98127aaa9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6f1e8f9ba1d45da87f0a9c10666bd5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FileUploadModel",
     "state": {
      "_counter": 1,
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FileUploadModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "FileUploadView",
      "accept": "image/*",
      "button_style": "",
      "data": [
       null
      ],
      "description": "Upload",
      "description_tooltip": null,
      "disabled": false,
      "error": "",
      "icon": "upload",
      "layout": "IPY_MODEL_95c660948d89422c95cefb98127aaa9b",
      "metadata": [
       {
        "lastModified": 1758109993801,
        "name": "dog_cnn_test.jpg",
        "size": 14058,
        "type": "image/jpeg"
       }
      ],
      "multiple": false,
      "style": "IPY_MODEL_ce4fc228205d4807bc1b856ab49442c0"
     }
    },
    "ce4fc228205d4807bc1b856ab49442c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
